{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cgN77/MIA-Class/blob/main/MIA_CLASS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## base"
      ],
      "metadata": {
        "id": "ttFzEMNZc4_v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1st ed"
      ],
      "metadata": {
        "id": "7xhB3wQ9c-Uq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### library"
      ],
      "metadata": {
        "id": "lKPMrYOqeF2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from typing import Optional, Dict, Any, Tuple, List\n",
        "import numpy as np\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "def entropy(probs):\n",
        "    return -(probs * probs.log()).sum().item()\n",
        "\n",
        "def margin_confidence(probs):\n",
        "    sorted_probs, _ = probs.sort(descending=True)\n",
        "    return (sorted_probs[0] - sorted_probs[1]).item()\n",
        "\n",
        "def top_k_sum(probs, k=3):\n",
        "    return probs.topk(k).values.sum().item()\n",
        "\n",
        "class AttackModel(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        \"\"\"\n",
        "        Initialize a simple feedforward neural network for the attack model.\n",
        "        Args:\n",
        "            input_size: The number of input features (number of classes in softmax output).\n",
        "        \"\"\"\n",
        "        super(AttackModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.sigmoid(self.fc3(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class LiRA:\n",
        "    def __init__(self, model_class, data_distribution, N=50):\n",
        "        \"\"\"\n",
        "        Initialize the LiRA attack.\n",
        "\n",
        "        Parameters:\n",
        "        - model_class: Class to initialize and train shadow models.\n",
        "        - data_distribution: Function to sample shadow datasets.\n",
        "        - N: Number of shadow models to train.\n",
        "        \"\"\"\n",
        "        self.model_class = model_class\n",
        "        self.data_distribution = data_distribution\n",
        "        self.N = N\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "    def train_shadow_model(self, dataset, model, epochs=5):\n",
        "        \"\"\"Train a shadow model on the given dataset.\"\"\"\n",
        "\n",
        "        model.to(self.device)\n",
        "        model.train()\n",
        "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        for epoch in range(epochs):  # Train for 5 epochs\n",
        "            for X, y in dataloader:\n",
        "                X, y = X.to(self.device), y.to(self.device)\n",
        "                optimizer.zero_grad()\n",
        "                predictions = model(X)\n",
        "                loss = criterion(predictions, y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        return model\n",
        "\n",
        "    def collect_confidences(self, dataset, target_example, is_in=True):\n",
        "        confidences = []\n",
        "        x, y = target_example\n",
        "        x, y = x.to(self.device), y.to(self.device)\n",
        "        def is_not_target(d, target):\n",
        "          return not (torch.equal(d[0], target[0]) and d[1] == target[1])\n",
        "\n",
        "\n",
        "        for _ in range(self.N):  # N should be >= 5 for meaningful variance\n",
        "            shadow_dataset = self.data_distribution()\n",
        "            # Convert data to tensors during dataset preparation\n",
        "            shadow_dataset = [(torch.tensor(x, dtype=torch.float32).to(self.device), torch.tensor(y, dtype=torch.long).to(self.device)) for x, y in shadow_dataset]\n",
        "\n",
        "            if is_in:\n",
        "                shadow_dataset.append(target_example)\n",
        "            else:\n",
        "                shadow_dataset = [d for d in shadow_dataset if is_not_target(d, target_example)]\n",
        "\n",
        "            # Train shadow model\n",
        "            shadow_model = self.model_class()\n",
        "            shadow_model = self.train_shadow_model(shadow_dataset, shadow_model)\n",
        "\n",
        "            # Get confidence of the target example\n",
        "            shadow_model.eval()\n",
        "            with torch.no_grad():\n",
        "                confidence = nn.Softmax(dim=1)(shadow_model(x.unsqueeze(0)))\n",
        "                confidences.append(confidence[0, y].item())\n",
        "\n",
        "        if len(confidences) < 2:\n",
        "            raise ValueError(\"Insufficient confidence values to compute meaningful statistics.\")\n",
        "        return confidences\n",
        "\n",
        "\n",
        "\n",
        "    def compute_statistics(self, confidences):\n",
        "        \"\"\"Compute mean and variance of confidence values.\"\"\"\n",
        "        mean = np.mean(confidences)\n",
        "        variance = np.var(confidences)\n",
        "        return mean, variance\n",
        "\n",
        "    def likelihood_ratio(self, conf_obs, mu_in, var_in, mu_out, var_out):\n",
        "        \"\"\"\n",
        "        Compute the likelihood ratio.\n",
        "\n",
        "        Parameters:\n",
        "        - conf_obs: Observed confidence value.\n",
        "        - mu_in, var_in: Mean and variance of IN distribution.\n",
        "        - mu_out, var_out: Mean and variance of OUT distribution.\n",
        "        \"\"\"\n",
        "        def gaussian_likelihood(x, mu, var):\n",
        "          epsilon = 1e-10  # Small constant to prevent zero variance\n",
        "          var = max(var, epsilon)  # Avoid division by zero\n",
        "          return (1 / np.sqrt(2 * np.pi * var)) * np.exp(-(x - mu) ** 2 / (2 * var))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        epsilon = 1e-10  # Small value to prevent division by zero\n",
        "        p_in = gaussian_likelihood(conf_obs, mu_in, var_in)\n",
        "        p_out = gaussian_likelihood(conf_obs, mu_out, var_out)\n",
        "\n",
        "\n",
        "        return p_in / (p_out + epsilon)\n",
        "\n",
        "    def run_attack(self, model, target_example):\n",
        "        \"\"\"\n",
        "        Run the LiRA attack on the target example.\n",
        "\n",
        "        Parameters:\n",
        "        - model: Target model.\n",
        "        - target_example: Example to attack (x, y).\n",
        "        \"\"\"\n",
        "        # Step 1: Collect IN and OUT confidences\n",
        "        model=model.to(self.device)\n",
        "        confs_in = self.collect_confidences(self.data_distribution, target_example, is_in=True)\n",
        "        confs_out = self.collect_confidences(self.data_distribution, target_example, is_in=False)\n",
        "\n",
        "        mu_in = np.mean(confs_in)\n",
        "        var_in = np.var(confs_in)\n",
        "\n",
        "        mu_out = np.mean(confs_out)\n",
        "        var_out = np.var(confs_out)\n",
        "\n",
        "\n",
        "        # Step 2: Compute IN and OUT statistics\n",
        "        mu_in, var_in = self.compute_statistics(confs_in)\n",
        "        mu_out, var_out = self.compute_statistics(confs_out)\n",
        "\n",
        "        # Step 3: Query the target model\n",
        "        x, y = target_example\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            conf_obs = nn.Softmax(dim=1)(model(x.unsqueeze(0)))[0, y].item()\n",
        "\n",
        "        # Step 4: Compute likelihood ratio\n",
        "        likelihood_ratio = self.likelihood_ratio(conf_obs, mu_in, var_in, mu_out, var_out)\n",
        "        return likelihood_ratio\n",
        "\n",
        "\n",
        "\n",
        "class GLiRA:\n",
        "    def __init__(self, model_class, data_distribution, N=50, likelihood_func=None, confidence_metric=\"softmax\", temperature=1.0):\n",
        "        \"\"\"\n",
        "        Initialize the GLiRA attack.\n",
        "\n",
        "        Parameters:\n",
        "        - model_class: Class to initialize and train shadow models.\n",
        "        - data_distribution: Function to sample shadow datasets (supports parameterization).\n",
        "        - N: Number of shadow models to train.\n",
        "        - likelihood_func: Custom likelihood function (default is Gaussian).\n",
        "        - confidence_metric: Metric for computing confidence (\"softmax\", \"logits\", or \"temperature\").\n",
        "        - temperature: Temperature parameter for scaling (default is 1.0).\n",
        "        \"\"\"\n",
        "        self.model_class = model_class\n",
        "        self.data_distribution = data_distribution\n",
        "        self.N = N\n",
        "        self.likelihood_func = likelihood_func or self.gaussian_likelihood\n",
        "        self.confidence_metric = confidence_metric\n",
        "        self.temperature = temperature\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    def train_shadow_model(self, dataset, model, epochs=5, batch_size=32, lr=0.001):\n",
        "        \"\"\"Train a shadow model on the given dataset.\"\"\"\n",
        "        model = model.to(self.device)\n",
        "        model.train()\n",
        "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            for X, y in dataloader:\n",
        "                X, y = X.to(self.device), y.to(self.device)\n",
        "                optimizer.zero_grad()\n",
        "                predictions = model(X)\n",
        "                loss = criterion(predictions, y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        return model\n",
        "\n",
        "    def compute_confidence(self, model, x, y):\n",
        "        \"\"\"Compute confidence using the specified metric.\"\"\"\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            logits = model(x.unsqueeze(0).to(self.device))\n",
        "            if self.confidence_metric == \"softmax\":\n",
        "                confidence = torch.nn.Softmax(dim=1)(logits)[0, y].item()\n",
        "            elif self.confidence_metric == \"logits\":\n",
        "                confidence = logits[0, y].item()\n",
        "            elif self.confidence_metric == \"temperature\":\n",
        "                scaled_logits = logits / self.temperature\n",
        "                confidence = torch.nn.Softmax(dim=1)(scaled_logits)[0, y].item()\n",
        "            elif self.confidence_metric == \"entropy\":\n",
        "                confidence = entropy(logits[0,y].item())\n",
        "            elif self.confidence_metric == \"margin\":\n",
        "                confidence = margin_confidence(logits[0,y].item())\n",
        "            elif self.confidence_metric == \"top_k\":\n",
        "                confidence = top_k_sum(logits[0,y].item())\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported confidence metric: {self.confidence_metric}\")\n",
        "        return confidence\n",
        "\n",
        "    def collect_confidences(self, target_example, is_in=True):\n",
        "        \"\"\"Collect confidence scores for shadow models.\"\"\"\n",
        "        confidences = []\n",
        "        x, y = target_example\n",
        "\n",
        "        def is_not_target(d, target):\n",
        "            return not (torch.equal(d[0], target[0]) and d[1] == target[1])\n",
        "\n",
        "        for _ in range(self.N):\n",
        "            shadow_dataset = self.data_distribution()\n",
        "            shadow_dataset = [(torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.long)) for x, y in shadow_dataset]\n",
        "\n",
        "            if is_in:\n",
        "                shadow_dataset.append(target_example)\n",
        "            else:\n",
        "                shadow_dataset = [d for d in shadow_dataset if is_not_target(d, target_example)]\n",
        "\n",
        "            shadow_model = self.model_class()\n",
        "            shadow_model = self.train_shadow_model(shadow_dataset, shadow_model)\n",
        "\n",
        "            confidence = self.compute_confidence(shadow_model, x, y)\n",
        "            confidences.append(confidence)\n",
        "\n",
        "        if len(confidences) < 2:\n",
        "            raise ValueError(\"Insufficient confidence values to compute meaningful statistics.\")\n",
        "        return confidences\n",
        "\n",
        "    def gaussian_likelihood(self, x, mu, var):\n",
        "        \"\"\"Default Gaussian likelihood function.\"\"\"\n",
        "        epsilon = 1e-10\n",
        "        var = max(var, epsilon)\n",
        "        return (1 / np.sqrt(2 * np.pi * var)) * np.exp(-(x - mu) ** 2 / (2 * var))\n",
        "\n",
        "    def compute_statistics(self, confidences):\n",
        "        \"\"\"Compute mean and variance of confidence values.\"\"\"\n",
        "        mean = np.mean(confidences)\n",
        "        variance = np.var(confidences)\n",
        "        return mean, variance\n",
        "\n",
        "    def likelihood_ratio(self, conf_obs, mu_in, var_in, mu_out, var_out):\n",
        "        \"\"\"Compute the likelihood ratio.\"\"\"\n",
        "        p_in = self.likelihood_func(conf_obs, mu_in, var_in)\n",
        "        p_out = self.likelihood_func(conf_obs, mu_out, var_out)\n",
        "\n",
        "        epsilon = 1e-10\n",
        "        return p_in / (p_out + epsilon)\n",
        "\n",
        "    def run_attack(self, model, target_example):\n",
        "        \"\"\"\n",
        "        Run the GLiRA attack on the target example.\n",
        "\n",
        "        Parameters:\n",
        "        - model: Target model.\n",
        "        - target_example: Example to attack (x, y).\n",
        "        \"\"\"\n",
        "        model = model.to(self.device)\n",
        "        confs_in = self.collect_confidences(target_example, is_in=True)\n",
        "        confs_out = self.collect_confidences(target_example, is_in=False)\n",
        "\n",
        "        mu_in, var_in = self.compute_statistics(confs_in)\n",
        "        mu_out, var_out = self.compute_statistics(confs_out)\n",
        "\n",
        "        x, y = target_example\n",
        "        conf_obs = self.compute_confidence(model, x, y)\n",
        "\n",
        "        likelihood_ratio = self.likelihood_ratio(conf_obs, mu_in, var_in, mu_out, var_out)\n",
        "        return likelihood_ratio\n",
        "\n",
        "\n",
        "class ShokriMembershipInference:\n",
        "    def __init__(self, shadow_model_class, data_distribution, num_shadow_models=5):\n",
        "        \"\"\"\n",
        "        Initialize the membership inference attack class.\n",
        "\n",
        "        Args:\n",
        "            shadow_model_class: A class to initialize shadow models.\n",
        "            data_distribution: A function to generate datasets for shadow models.\n",
        "            num_shadow_models: Number of shadow models to train.\n",
        "        \"\"\"\n",
        "        self.shadow_model_class = shadow_model_class\n",
        "        self.data_distribution = data_distribution\n",
        "        self.num_shadow_models = num_shadow_models\n",
        "        self.attack_model = None\n",
        "\n",
        "    def train_shadow_models(self, train_data):\n",
        "        \"\"\"\n",
        "        Train a single shadow model.\n",
        "\n",
        "        Args:\n",
        "            train_data: Dataset to train the shadow model.\n",
        "\n",
        "        Returns:\n",
        "            Trained shadow model.\n",
        "        \"\"\"\n",
        "        model = self.shadow_model_class()\n",
        "        model.train()\n",
        "        dataloader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        for epoch in range(10):  # Increased epochs to improve training\n",
        "            for X, y in dataloader:\n",
        "                optimizer.zero_grad()\n",
        "                predictions = model(X)\n",
        "                loss = criterion(predictions, y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        return model\n",
        "\n",
        "    def build_attack_dataset(self):\n",
        "        \"\"\"\n",
        "        Build the attack dataset by training shadow models and collecting their outputs.\n",
        "\n",
        "        Returns:\n",
        "            X_attack: Features for the attack model (softmax output vectors).\n",
        "            y_attack: Labels for the attack model (1 for members, 0 for non-members).\n",
        "        \"\"\"\n",
        "        X_attack = []\n",
        "        y_attack = []\n",
        "\n",
        "        for _ in range(self.num_shadow_models):\n",
        "            shadow_data = self.data_distribution()\n",
        "            shadow_train, shadow_test = train_test_split(shadow_data, test_size=0.2, random_state=42)\n",
        "\n",
        "            # Train shadow model on shadow_train\n",
        "            shadow_model = self.train_shadow_models(shadow_train)\n",
        "\n",
        "            # Collect softmax outputs for members (training set)\n",
        "            for X, y in shadow_train:\n",
        "                X = torch.tensor(X, dtype=torch.float32)  # Convert X to tensor\n",
        "                y = torch.tensor(y, dtype=torch.long)     # Convert y to tensor\n",
        "                shadow_model.eval()\n",
        "                with torch.no_grad():\n",
        "                    softmax_outputs = F.softmax(shadow_model(X.unsqueeze(0)), dim=1)\n",
        "                    entropy = -torch.sum(softmax_outputs * torch.log(softmax_outputs), dim=1).item()\n",
        "                    confidence_gap = (softmax_outputs.topk(2).values[:, 0] - softmax_outputs.topk(2).values[:, 1]).item()\n",
        "                    features = softmax_outputs.squeeze(0).tolist() + [entropy, confidence_gap]\n",
        "                    X_attack.append(features)\n",
        "                    y_attack.append(1)  # Member\n",
        "\n",
        "            # Collect softmax outputs for non-members (testing set)\n",
        "            for X, y in shadow_test:\n",
        "                X = torch.tensor(X, dtype=torch.float32)  # Convert X to tensor\n",
        "                y = torch.tensor(y, dtype=torch.long)     # Convert y to tensor\n",
        "                shadow_model.eval()\n",
        "                with torch.no_grad():\n",
        "                    softmax_outputs = F.softmax(shadow_model(X.unsqueeze(0)), dim=1)\n",
        "                    entropy = -torch.sum(softmax_outputs * torch.log(softmax_outputs), dim=1).item()\n",
        "                    confidence_gap = (softmax_outputs.topk(2).values[:, 0] - softmax_outputs.topk(2).values[:, 1]).item()\n",
        "                    features = softmax_outputs.squeeze(0).tolist() + [entropy, confidence_gap]\n",
        "                    X_attack.append(features)\n",
        "                    y_attack.append(0)  # Non-member\n",
        "\n",
        "        return X_attack, y_attack\n",
        "\n",
        "\n",
        "    def train_attack_model(self):\n",
        "        \"\"\"\n",
        "        Train the attack model using the attack dataset.\n",
        "\n",
        "        Returns:\n",
        "            Trained attack model.\n",
        "        \"\"\"\n",
        "        # Build the attack dataset\n",
        "        X_attack, y_attack = self.build_attack_dataset()\n",
        "\n",
        "        # Balance the dataset\n",
        "        rus = RandomUnderSampler(random_state=42)\n",
        "        X_attack, y_attack = rus.fit_resample(X_attack, y_attack)\n",
        "\n",
        "        # Split into training and validation sets\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X_attack, y_attack, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Convert data to PyTorch tensors\n",
        "        X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "        y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
        "        X_val = torch.tensor(X_val, dtype=torch.float32)\n",
        "        y_val = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "        # Define the attack model\n",
        "        input_size = X_train.shape[1]\n",
        "        attack_model = AttackModel(input_size)\n",
        "        criterion = nn.BCELoss()\n",
        "        optimizer = torch.optim.Adam(attack_model.parameters(), lr=0.001)\n",
        "\n",
        "        # Train the attack model\n",
        "        epochs = 20\n",
        "        for epoch in range(epochs):\n",
        "            attack_model.train()\n",
        "            optimizer.zero_grad()\n",
        "            outputs = attack_model(X_train)\n",
        "            loss = criterion(outputs, y_train)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Print loss for every epoch\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "        # Validate the attack model\n",
        "        attack_model.eval()\n",
        "        with torch.no_grad():\n",
        "            y_pred = attack_model(X_val).squeeze().round()\n",
        "            accuracy = accuracy_score(y_val.numpy(), y_pred.numpy())\n",
        "            print(f\"Attack Model Validation Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "        self.attack_model = attack_model\n",
        "\n",
        "    def perform_inference(self, target_model, samples):\n",
        "        \"\"\"\n",
        "        Perform batch membership inference on a list of samples using the trained attack model.\n",
        "\n",
        "        Args:\n",
        "            target_model: The target model to attack.\n",
        "            samples: A batch of samples (as a tuple (X, y)).\n",
        "\n",
        "        Returns:\n",
        "            Membership inference results for the batch (1 for member, 0 for non-member).\n",
        "        \"\"\"\n",
        "        if self.attack_model is None:\n",
        "            raise ValueError(\"Attack model has not been trained. Call train_attack_model() first.\")\n",
        "\n",
        "        X, _ = samples  # Unpack samples\n",
        "        X = torch.tensor(X, dtype=torch.float32)  # Convert X to tensor\n",
        "\n",
        "        target_model.eval()\n",
        "        with torch.no_grad():\n",
        "            # Get softmax outputs for the batch\n",
        "            softmax_outputs = F.softmax(target_model(X), dim=1)\n",
        "            entropy = -torch.sum(softmax_outputs * torch.log(softmax_outputs), dim=1)\n",
        "            confidence_gap = softmax_outputs.topk(2).values[:, 0] - softmax_outputs.topk(2).values[:, 1]\n",
        "            features = torch.cat([softmax_outputs, entropy.unsqueeze(1), confidence_gap.unsqueeze(1)], dim=1)\n",
        "\n",
        "        # Use the attack model to predict membership\n",
        "        self.attack_model.eval()\n",
        "        with torch.no_grad():\n",
        "            membership_predictions = self.attack_model(features).squeeze().round()\n",
        "\n",
        "        return membership_predictions.numpy()\n",
        "\n",
        "\n",
        "\n",
        "class MembershipInferenceAttack:\n",
        "    def __init__(self,\n",
        "                 target_model: nn.Module,\n",
        "                 target_class:nn.Module,\n",
        "                 dataset: torch.utils.data.Dataset,\n",
        "                 data_distribution:callable = None,\n",
        "                 attack_model_type: str = 'LiRA',\n",
        "                 device: Optional[str] = 'cpu',\n",
        "                 hyperparameters: Optional[Dict[str, Any]] = None,\n",
        "                 shadow_models_count = 10):\n",
        "        \"\"\"\n",
        "        Initializes the membership inference attack.\n",
        "\n",
        "        Args:\n",
        "            target_model: The target model to attack.\n",
        "            dataset: Dataset containing samples to evaluate.\n",
        "            attack_model_type: The attack type ('LiRA', 'MALT', 'MAST').\n",
        "            device: Device to run computations on ('cpu' or 'cuda').\n",
        "            hyperparameters: Additional parameters for the attack models.\n",
        "        \"\"\"\n",
        "        self.target_model = target_model.to(device)\n",
        "        self.target_class = target_class\n",
        "        self.dataset = dataset\n",
        "        self.attack_model_type = attack_model_type\n",
        "        self.device = device\n",
        "        self.hyperparameters = hyperparameters if hyperparameters else {}\n",
        "        self.shadow_models_count = shadow_models_count\n",
        "        self.data_distribution = data_distribution\n",
        "\n",
        "        # Attack-specific parameters\n",
        "        self.num_perturbations = self.hyperparameters.get('num_perturbations', 100)\n",
        "        self.epsilon = self.hyperparameters.get('epsilon', 0.01)\n",
        "        self.augmentation_strategy = self.hyperparameters.get('augmentation_strategy', 'default')\n",
        "\n",
        "        # Member and non-member datasets\n",
        "        self.members = []\n",
        "        self.non_members = []\n",
        "\n",
        "        # Placeholder for attack-specific model\n",
        "        self.attack_model = None\n",
        "        self.i = 0\n",
        "    ### DATA PROCESSING ###\n",
        "    def prepare_datasets(self, members, non_members):\n",
        "        \"\"\"\n",
        "        Prepares datasets for members and non-members.\n",
        "\n",
        "        Args:\n",
        "            members: Member samples.\n",
        "            non_members: Non-member samples.\n",
        "        \"\"\"\n",
        "        self.members = [(sample, label) for sample, label in members]\n",
        "        self.non_members = [(sample, label) for sample, label in non_members]\n",
        "\n",
        "    def augment_data(self, sample: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Augments the data based on the selected strategy.\n",
        "\n",
        "        Args:\n",
        "            sample: A single input sample.\n",
        "\n",
        "        Returns:\n",
        "            Augmented samples.\n",
        "        \"\"\"\n",
        "        if self.augmentation_strategy == 'noise':\n",
        "            noise = torch.randn(sample.shape, device=self.device) * self.epsilon\n",
        "            return sample + noise\n",
        "        elif self.augmentation_strategy == 'scaling':\n",
        "            scale = 1 + (torch.randn(1, device=self.device) * self.epsilon)\n",
        "            return sample * scale\n",
        "        else:\n",
        "            return sample  # Default: No augmentation\n",
        "\n",
        "    ### ATTACK MODEL CONFIGURATION ###\n",
        "    def configure_attack_model(self):\n",
        "        \"\"\"\n",
        "        Configures the attack model based on the chosen attack type.\n",
        "        \"\"\"\n",
        "        if self.attack_model_type == 'LiRA':\n",
        "            print(\"Configuring LiRA attack...\")\n",
        "            self.attack_model = self._lira_attack\n",
        "        elif self.attack_model_type == 'GLIRA':\n",
        "            print(\"Configuring GLiRA attack...\")\n",
        "            self.attack_model = self._glira_attack\n",
        "        elif self.attack_model_type=='shokri':\n",
        "            print(\"Configuring Shokri attack...\")\n",
        "            self.attack_model = self._shokri_attack\n",
        "        elif self.attack_model_type == 'MAST':\n",
        "            print(\"Configuring MAST attack...\")\n",
        "            self.attack_model = self._mast_attack\n",
        "        elif self.attack_model_type == 'MALT':\n",
        "            print(\"Configuring MALT attack...\")\n",
        "            self.attack_model = self._malt_attack\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported attack model type: {self.attack_model_type}\")\n",
        "\n",
        "    ### LIRA ATTACK ###\n",
        "    def _lira_attack(self, sample: torch.Tensor) -> float:\n",
        "        \"\"\"\n",
        "        Performs the LiRA attack on a single sample using the LiRA class.\n",
        "\n",
        "        Args:\n",
        "            sample: A single input sample.\n",
        "            label: The true label for the sample.\n",
        "\n",
        "        Returns:\n",
        "            Likelihood ratio score.\n",
        "        \"\"\"\n",
        "        self.target_model.eval()\n",
        "\n",
        "        # Prepare the target example (sample, label)\n",
        "        x = sample[0].to(self.device)\n",
        "        y= torch.tensor(sample[1]).to(self.device)\n",
        "\n",
        "        # Run the LiRA attack\n",
        "        lira = LiRA(model_class=self.target_class, data_distribution=self.data_distribution, N=self.shadow_models_count)\n",
        "\n",
        "        # Run the LiRA attack\n",
        "        likelihood_ratio = lira.run_attack(target_model, (x,y))\n",
        "\n",
        "        return likelihood_ratio\n",
        "\n",
        "    ### GLiRA ATTACK ###\n",
        "    def _glira_attack(self, sample: torch.Tensor, likelihood_func, metric: callable = None) -> float:\n",
        "        \"\"\"\n",
        "        Performs the GLiRA (Generalized Likelihood Ratio Attack) on a single sample\n",
        "        using a specified metric for likelihood comparison.\n",
        "\n",
        "        Args:\n",
        "            sample: A single input sample.\n",
        "            metric: A callable function to calculate the desired metric\n",
        "                    (e.g., max probability, entropy, or margin confidence).\n",
        "                    Defaults to using the max probability.\n",
        "\n",
        "        Returns:\n",
        "            Generalized likelihood ratio score.\n",
        "        \"\"\"\n",
        "        self.target_model.eval()\n",
        "        x=sample[0].to(self.device)\n",
        "        y=torch.tensor(sample[1]).to(self.device)\n",
        "        glira=GLiRA(self.target_class,self.dataset,self.shadow_models_count, likelihood_func,metric)\n",
        "        likelihood_ratio=glira.run_attack(self.target_model,(x,y))\n",
        "        return likelihood_ratio\n",
        "\n",
        "    def _shokri_attack(self, sample: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Performs the Shokri attack on a single sample.\n",
        "\n",
        "        Args:\n",
        "            sample: A single input sample.\n",
        "\n",
        "        Returns:\n",
        "            Membership score.\n",
        "        \"\"\"\n",
        "        self.target_model.eval()\n",
        "        x = sample[0].to(self.device)\n",
        "        y= torch.tensor(sample[1]).to(self.device)\n",
        "\n",
        "        attack = ShokriMembershipInference(\n",
        "            shadow_model_class=self.target_class,\n",
        "            data_distribution=self.data_distribution,\n",
        "            num_shadow_models=self.shadow_models_count\n",
        "        )\n",
        "\n",
        "        attack.train_shadow_models(self.dataset)\n",
        "        attack.train_attack_model()\n",
        "\n",
        "        result = attack.perform_inference(self.target_model,(x,y))\n",
        "        return result\n",
        "\n",
        "    ### MAST ATTACK ###\n",
        "    def _mast_attack(self, sample: torch.Tensor) -> float:\n",
        "        \"\"\"\n",
        "        Performs the MAST attack on a single sample.\n",
        "\n",
        "        Args:\n",
        "            sample: A single input sample.\n",
        "\n",
        "        Returns:\n",
        "            Membership score.\n",
        "        \"\"\"\n",
        "        self.target_model.eval()\n",
        "        with torch.no_grad():\n",
        "            original_output = self.target_model(sample.unsqueeze(0).to(self.device))\n",
        "            logits = original_output.squeeze()\n",
        "            softmax_scores = F.softmax(logits, dim=-1)\n",
        "            return torch.norm(logits - softmax_scores).item()\n",
        "\n",
        "    ### MALT ATTACK ###\n",
        "    def _malt_attack(self, sample: torch.Tensor) -> float:\n",
        "        \"\"\"\n",
        "        Performs the MALT attack on a single sample.\n",
        "\n",
        "        Args:\n",
        "            sample: A single input sample.\n",
        "\n",
        "        Returns:\n",
        "            Membership score.\n",
        "        \"\"\"\n",
        "        self.target_model.eval()\n",
        "        with torch.no_grad():\n",
        "            original_output = self.target_model(sample.unsqueeze(0).to(self.device))\n",
        "            logits = original_output.squeeze()\n",
        "            confidence = F.softmax(logits, dim=-1).max().item()\n",
        "            entropy = -torch.sum(F.softmax(logits, dim=-1) * F.log_softmax(logits, dim=-1)).item()\n",
        "            return confidence - entropy\n",
        "\n",
        "    ### PERTURBATION GENERATION ###\n",
        "    def generate_perturbations(self, sample: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Generate perturbations for the given sample.\n",
        "\n",
        "        Args:\n",
        "            sample: A single input sample.\n",
        "\n",
        "        Returns:\n",
        "            A tensor containing perturbed samples.\n",
        "        \"\"\"\n",
        "        # Example perturbation logic: Add small random noise\n",
        "        num_perturbations = 10\n",
        "        noise = torch.randn((num_perturbations,) + sample.shape, device=self.device) * 0.01\n",
        "        return sample.unsqueeze(0).repeat(num_perturbations, 1) + noise\n",
        "\n",
        "    ### MEMBERSHIP INFERENCE ###\n",
        "    def infer_membership(self, sample: torch.Tensor) -> int:\n",
        "        \"\"\"\n",
        "        Infers the membership status of a given sample.\n",
        "\n",
        "        Args:\n",
        "            sample: A single input sample.\n",
        "\n",
        "        Returns:\n",
        "            1 if the sample is inferred as a member, 0 otherwise.\n",
        "        \"\"\"\n",
        "\n",
        "        print(self.i)\n",
        "        self.i += 1\n",
        "        if self.attack_model_type=='shokri':\n",
        "            score=self.attack_model(sample)\n",
        "            return score\n",
        "        if self.attack_model_type == 'LiRA' or self.attack_model_type=='GLiRA':\n",
        "            score = self.attack_model(sample)\n",
        "            return 1 if score >= 7 else 0\n",
        "        elif self.attack_model_type in {'MAST', 'MALT'}:\n",
        "            score = self.attack_model(sample)\n",
        "            return 1 if score > 0.5 else 0\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported attack type: {self.attack_model_type}\")\n",
        "\n",
        "    ### EVALUATION ###\n",
        "    def evaluate_attack(self) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Evaluates the attack on members and non-members datasets.\n",
        "\n",
        "        Returns:\n",
        "            A dictionary containing evaluation metrics.\n",
        "        \"\"\"\n",
        "        y_true = [1] * len(self.members) + [0] * len(self.non_members)\n",
        "        y_pred = [self.infer_membership(sample) for sample in (self.members + self.non_members)]\n",
        "\n",
        "        # Compute evaluation metrics\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        precision = precision_score(y_true, y_pred)\n",
        "        recall = recall_score(y_true, y_pred)\n",
        "        f1 = f1_score(y_true, y_pred)\n",
        "        roc_auc = roc_auc_score(y_true, y_pred)\n",
        "        print(f\"y_true:{y_true}\" )\n",
        "        print(f\"y_pred:{y_pred}\" )\n",
        "        return {\n",
        "            \"accuracy\": accuracy,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"f1_score\": f1,\n",
        "            \"roc_auc\": roc_auc\n",
        "        }\n"
      ],
      "metadata": {
        "id": "TPi_MXhKKXa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade sympy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tobykeNT5CVF",
        "outputId": "4ccaca76-929b-4f3b-87ea-9d4e14e1f0f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (1.13.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### example use cases"
      ],
      "metadata": {
        "id": "PAy2BYsxeKPF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### lira use case"
      ],
      "metadata": {
        "id": "JcB6r_pSemUt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### importing libs"
      ],
      "metadata": {
        "id": "wtMYVgDAeVqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "HH1_vGw2eVXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### dataset prep"
      ],
      "metadata": {
        "id": "8kOJEz-reZbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset preparation\n",
        "transform = transforms.ToTensor()\n",
        "dataset = MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "# print(len(dataset))\n",
        "members, non_members = torch.utils.data.random_split(dataset, [30000, 30000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdWv4Kh_eyjg",
        "outputId": "2c97a5bb-3bd7-4ba7-a535-8a4754661c2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 16.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 488kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.43MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 2.36MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### target model prep"
      ],
      "metadata": {
        "id": "XL_c5mLRe1Bq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#target model architecture\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size=784, num_classes=10):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x.view(x.size(0), -1))\n",
        "\n",
        "#target model training\n",
        "def train_target_model(model, dataset, epochs=5):\n",
        "    model.train()\n",
        "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            predictions = model(X)\n",
        "            loss = criterion(predictions, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    return model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "target_model = SimpleNN().to(device)\n",
        "target_model = train_target_model(target_model, members)"
      ],
      "metadata": {
        "id": "09KlVhRJe3rt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### mia attack"
      ],
      "metadata": {
        "id": "MYOTMYKfghs-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_distribution():\n",
        "    \"\"\"\n",
        "    Generate a random shadow dataset from MNIST.\n",
        "    \"\"\"\n",
        "    dataset = datasets.MNIST(root=\"./data\", train=True, transform=transforms.ToTensor(), download=True)\n",
        "    indices = np.random.choice(len(dataset), 500, replace=False)\n",
        "    subset = torch.utils.data.Subset(dataset, indices)\n",
        "    return list(subset)"
      ],
      "metadata": {
        "id": "Vtm9UjIH-W3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "shokri attack"
      ],
      "metadata": {
        "id": "4i-i4bBg-wnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "mia = MembershipInferenceAttack(\n",
        "    target_model=target_model,\n",
        "    target_class=SimpleNN,\n",
        "    dataset=dataset,\n",
        "    data_distribution=data_distribution,\n",
        "    shadow_models_count=5,\n",
        "    attack_model_type=\"shokri\",\n",
        "    device=device,\n",
        "    hyperparameters={\n",
        "        'num_perturbations': 50,\n",
        "        'epsilon': 0.01,\n",
        "        'augmentation_strategy': 'noise'\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "memb2 = torch.utils.data.Subset(members, list(range(5)))\n",
        "non_memb2 = torch.utils.data.Subset(non_members, list(range(5)))\n",
        "mia.prepare_datasets(memb2,non_memb2)\n",
        "\n",
        "\n",
        "mia.configure_attack_model()\n",
        "\n",
        "\n",
        "results = mia.evaluate_attack()\n",
        "\n",
        "\n",
        "print(\"Attack Evaluation Results:\")\n",
        "for metric, value in results.items():\n",
        "    print(f\"{metric}: {value:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zvht-xWT-vAk",
        "outputId": "a82b273f-cd39-4183-f140-0c8714f38294"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuring Shokri attack...\n",
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-2171e4d8ca2c>:370: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X = torch.tensor(X, dtype=torch.float32)  # Convert X to tensor\n",
            "<ipython-input-12-2171e4d8ca2c>:383: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X = torch.tensor(X, dtype=torch.float32)  # Convert X to tensor\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
            "  warnings.warn(\n",
            "<ipython-input-12-2171e4d8ca2c>:463: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X = torch.tensor(X, dtype=torch.float32)  # Convert X to tensor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 0.6993\n",
            "Epoch 2/20, Loss: 0.6982\n",
            "Epoch 3/20, Loss: 0.6972\n",
            "Epoch 4/20, Loss: 0.6963\n",
            "Epoch 5/20, Loss: 0.6954\n",
            "Epoch 6/20, Loss: 0.6946\n",
            "Epoch 7/20, Loss: 0.6938\n",
            "Epoch 8/20, Loss: 0.6931\n",
            "Epoch 9/20, Loss: 0.6924\n",
            "Epoch 10/20, Loss: 0.6918\n",
            "Epoch 11/20, Loss: 0.6911\n",
            "Epoch 12/20, Loss: 0.6905\n",
            "Epoch 13/20, Loss: 0.6900\n",
            "Epoch 14/20, Loss: 0.6894\n",
            "Epoch 15/20, Loss: 0.6888\n",
            "Epoch 16/20, Loss: 0.6882\n",
            "Epoch 17/20, Loss: 0.6877\n",
            "Epoch 18/20, Loss: 0.6872\n",
            "Epoch 19/20, Loss: 0.6866\n",
            "Epoch 20/20, Loss: 0.6861\n",
            "Attack Model Validation Accuracy: 57.00%\n",
            "1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-2171e4d8ca2c>:370: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X = torch.tensor(X, dtype=torch.float32)  # Convert X to tensor\n",
            "<ipython-input-12-2171e4d8ca2c>:383: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X = torch.tensor(X, dtype=torch.float32)  # Convert X to tensor\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
            "  warnings.warn(\n",
            "<ipython-input-12-2171e4d8ca2c>:463: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X = torch.tensor(X, dtype=torch.float32)  # Convert X to tensor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 0.6931\n",
            "Epoch 2/20, Loss: 0.6926\n",
            "Epoch 3/20, Loss: 0.6921\n",
            "Epoch 4/20, Loss: 0.6916\n",
            "Epoch 5/20, Loss: 0.6912\n",
            "Epoch 6/20, Loss: 0.6908\n",
            "Epoch 7/20, Loss: 0.6904\n",
            "Epoch 8/20, Loss: 0.6900\n",
            "Epoch 9/20, Loss: 0.6896\n",
            "Epoch 10/20, Loss: 0.6892\n",
            "Epoch 11/20, Loss: 0.6889\n",
            "Epoch 12/20, Loss: 0.6885\n",
            "Epoch 13/20, Loss: 0.6881\n",
            "Epoch 14/20, Loss: 0.6877\n",
            "Epoch 15/20, Loss: 0.6874\n",
            "Epoch 16/20, Loss: 0.6870\n",
            "Epoch 17/20, Loss: 0.6866\n",
            "Epoch 18/20, Loss: 0.6862\n",
            "Epoch 19/20, Loss: 0.6858\n",
            "Epoch 20/20, Loss: 0.6855\n",
            "Attack Model Validation Accuracy: 60.50%\n",
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-2171e4d8ca2c>:370: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X = torch.tensor(X, dtype=torch.float32)  # Convert X to tensor\n",
            "<ipython-input-12-2171e4d8ca2c>:383: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X = torch.tensor(X, dtype=torch.float32)  # Convert X to tensor\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
            "  warnings.warn(\n",
            "<ipython-input-12-2171e4d8ca2c>:463: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X = torch.tensor(X, dtype=torch.float32)  # Convert X to tensor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 0.6935\n",
            "Epoch 2/20, Loss: 0.6930\n",
            "Epoch 3/20, Loss: 0.6925\n",
            "Epoch 4/20, Loss: 0.6920\n",
            "Epoch 5/20, Loss: 0.6916\n",
            "Epoch 6/20, Loss: 0.6912\n",
            "Epoch 7/20, Loss: 0.6908\n",
            "Epoch 8/20, Loss: 0.6904\n",
            "Epoch 9/20, Loss: 0.6900\n",
            "Epoch 10/20, Loss: 0.6896\n",
            "Epoch 11/20, Loss: 0.6892\n",
            "Epoch 12/20, Loss: 0.6889\n",
            "Epoch 13/20, Loss: 0.6885\n",
            "Epoch 14/20, Loss: 0.6881\n",
            "Epoch 15/20, Loss: 0.6877\n",
            "Epoch 16/20, Loss: 0.6872\n",
            "Epoch 17/20, Loss: 0.6868\n",
            "Epoch 18/20, Loss: 0.6864\n",
            "Epoch 19/20, Loss: 0.6860\n",
            "Epoch 20/20, Loss: 0.6855\n",
            "Attack Model Validation Accuracy: 53.50%\n",
            "3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-2171e4d8ca2c>:370: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X = torch.tensor(X, dtype=torch.float32)  # Convert X to tensor\n",
            "<ipython-input-12-2171e4d8ca2c>:383: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X = torch.tensor(X, dtype=torch.float32)  # Convert X to tensor\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
            "  warnings.warn(\n",
            "<ipython-input-12-2171e4d8ca2c>:463: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X = torch.tensor(X, dtype=torch.float32)  # Convert X to tensor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 0.6957\n",
            "Epoch 2/20, Loss: 0.6952\n",
            "Epoch 3/20, Loss: 0.6947\n",
            "Epoch 4/20, Loss: 0.6943\n",
            "Epoch 5/20, Loss: 0.6939\n",
            "Epoch 6/20, Loss: 0.6935\n",
            "Epoch 7/20, Loss: 0.6931\n",
            "Epoch 8/20, Loss: 0.6928\n",
            "Epoch 9/20, Loss: 0.6924\n",
            "Epoch 10/20, Loss: 0.6921\n",
            "Epoch 11/20, Loss: 0.6918\n",
            "Epoch 12/20, Loss: 0.6915\n",
            "Epoch 13/20, Loss: 0.6912\n",
            "Epoch 14/20, Loss: 0.6909\n",
            "Epoch 15/20, Loss: 0.6906\n",
            "Epoch 16/20, Loss: 0.6903\n",
            "Epoch 17/20, Loss: 0.6900\n",
            "Epoch 18/20, Loss: 0.6897\n",
            "Epoch 19/20, Loss: 0.6894\n",
            "Epoch 20/20, Loss: 0.6891\n",
            "Attack Model Validation Accuracy: 56.00%\n",
            "4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-2171e4d8ca2c>:370: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X = torch.tensor(X, dtype=torch.float32)  # Convert X to tensor\n",
            "<ipython-input-12-2171e4d8ca2c>:383: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X = torch.tensor(X, dtype=torch.float32)  # Convert X to tensor\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
            "  warnings.warn(\n",
            "<ipython-input-12-2171e4d8ca2c>:463: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X = torch.tensor(X, dtype=torch.float32)  # Convert X to tensor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 0.6926\n",
            "Epoch 2/20, Loss: 0.6921\n",
            "Epoch 3/20, Loss: 0.6917\n",
            "Epoch 4/20, Loss: 0.6912\n",
            "Epoch 5/20, Loss: 0.6907\n",
            "Epoch 6/20, Loss: 0.6902\n",
            "Epoch 7/20, Loss: 0.6897\n",
            "Epoch 8/20, Loss: 0.6892\n",
            "Epoch 9/20, Loss: 0.6887\n",
            "Epoch 10/20, Loss: 0.6881\n",
            "Epoch 11/20, Loss: 0.6876\n",
            "Epoch 12/20, Loss: 0.6870\n",
            "Epoch 13/20, Loss: 0.6865\n",
            "Epoch 14/20, Loss: 0.6859\n",
            "Epoch 15/20, Loss: 0.6853\n",
            "Epoch 16/20, Loss: 0.6848\n",
            "Epoch 17/20, Loss: 0.6842\n",
            "Epoch 18/20, Loss: 0.6835\n",
            "Epoch 19/20, Loss: 0.6829\n",
            "Epoch 20/20, Loss: 0.6822\n",
            "Attack Model Validation Accuracy: 58.00%\n",
            "5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-2171e4d8ca2c>:370: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X = torch.tensor(X, dtype=torch.float32)  # Convert X to tensor\n",
            "<ipython-input-12-2171e4d8ca2c>:383: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X = torch.tensor(X, dtype=torch.float32)  # Convert X to tensor\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
            "  warnings.warn(\n",
            "<ipython-input-12-2171e4d8ca2c>:463: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X = torch.tensor(X, dtype=torch.float32)  # Convert X to tensor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 0.6948\n",
            "Epoch 2/20, Loss: 0.6942\n",
            "Epoch 3/20, Loss: 0.6936\n",
            "Epoch 4/20, Loss: 0.6930\n",
            "Epoch 5/20, Loss: 0.6925\n",
            "Epoch 6/20, Loss: 0.6920\n",
            "Epoch 7/20, Loss: 0.6914\n",
            "Epoch 8/20, Loss: 0.6909\n",
            "Epoch 9/20, Loss: 0.6904\n",
            "Epoch 10/20, Loss: 0.6899\n",
            "Epoch 11/20, Loss: 0.6895\n",
            "Epoch 12/20, Loss: 0.6890\n",
            "Epoch 13/20, Loss: 0.6885\n",
            "Epoch 14/20, Loss: 0.6880\n",
            "Epoch 15/20, Loss: 0.6875\n",
            "Epoch 16/20, Loss: 0.6870\n",
            "Epoch 17/20, Loss: 0.6865\n",
            "Epoch 18/20, Loss: 0.6860\n",
            "Epoch 19/20, Loss: 0.6855\n",
            "Epoch 20/20, Loss: 0.6849\n",
            "Attack Model Validation Accuracy: 58.50%\n",
            "6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-2171e4d8ca2c>:370: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X = torch.tensor(X, dtype=torch.float32)  # Convert X to tensor\n",
            "<ipython-input-12-2171e4d8ca2c>:383: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X = torch.tensor(X, dtype=torch.float32)  # Convert X to tensor\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
            "  warnings.warn(\n",
            "<ipython-input-12-2171e4d8ca2c>:463: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X = torch.tensor(X, dtype=torch.float32)  # Convert X to tensor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 0.6922\n",
            "Epoch 2/20, Loss: 0.6913\n",
            "Epoch 3/20, Loss: 0.6905\n",
            "Epoch 4/20, Loss: 0.6898\n",
            "Epoch 5/20, Loss: 0.6890\n",
            "Epoch 6/20, Loss: 0.6883\n",
            "Epoch 7/20, Loss: 0.6876\n",
            "Epoch 8/20, Loss: 0.6870\n",
            "Epoch 9/20, Loss: 0.6863\n",
            "Epoch 10/20, Loss: 0.6857\n",
            "Epoch 11/20, Loss: 0.6851\n",
            "Epoch 12/20, Loss: 0.6845\n",
            "Epoch 13/20, Loss: 0.6840\n",
            "Epoch 14/20, Loss: 0.6834\n",
            "Epoch 15/20, Loss: 0.6828\n",
            "Epoch 16/20, Loss: 0.6822\n",
            "Epoch 17/20, Loss: 0.6816\n",
            "Epoch 18/20, Loss: 0.6810\n",
            "Epoch 19/20, Loss: 0.6803\n",
            "Epoch 20/20, Loss: 0.6796\n",
            "Attack Model Validation Accuracy: 57.00%\n",
            "7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-2171e4d8ca2c>:370: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X = torch.tensor(X, dtype=torch.float32)  # Convert X to tensor\n",
            "<ipython-input-12-2171e4d8ca2c>:383: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X = torch.tensor(X, dtype=torch.float32)  # Convert X to tensor\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
            "  warnings.warn(\n",
            "<ipython-input-12-2171e4d8ca2c>:463: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X = torch.tensor(X, dtype=torch.float32)  # Convert X to tensor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 0.6953\n",
            "Epoch 2/20, Loss: 0.6942\n",
            "Epoch 3/20, Loss: 0.6932\n",
            "Epoch 4/20, Loss: 0.6922\n",
            "Epoch 5/20, Loss: 0.6913\n",
            "Epoch 6/20, Loss: 0.6904\n",
            "Epoch 7/20, Loss: 0.6895\n",
            "Epoch 8/20, Loss: 0.6888\n",
            "Epoch 9/20, Loss: 0.6880\n",
            "Epoch 10/20, Loss: 0.6873\n",
            "Epoch 11/20, Loss: 0.6865\n",
            "Epoch 12/20, Loss: 0.6858\n",
            "Epoch 13/20, Loss: 0.6851\n",
            "Epoch 14/20, Loss: 0.6843\n",
            "Epoch 15/20, Loss: 0.6836\n",
            "Epoch 16/20, Loss: 0.6828\n",
            "Epoch 17/20, Loss: 0.6821\n",
            "Epoch 18/20, Loss: 0.6813\n",
            "Epoch 19/20, Loss: 0.6806\n",
            "Epoch 20/20, Loss: 0.6798\n",
            "Attack Model Validation Accuracy: 57.00%\n",
            "8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-2171e4d8ca2c>:370: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X = torch.tensor(X, dtype=torch.float32)  # Convert X to tensor\n",
            "<ipython-input-12-2171e4d8ca2c>:383: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X = torch.tensor(X, dtype=torch.float32)  # Convert X to tensor\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
            "  warnings.warn(\n",
            "<ipython-input-12-2171e4d8ca2c>:463: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X = torch.tensor(X, dtype=torch.float32)  # Convert X to tensor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 0.6967\n",
            "Epoch 2/20, Loss: 0.6957\n",
            "Epoch 3/20, Loss: 0.6947\n",
            "Epoch 4/20, Loss: 0.6938\n",
            "Epoch 5/20, Loss: 0.6930\n",
            "Epoch 6/20, Loss: 0.6923\n",
            "Epoch 7/20, Loss: 0.6916\n",
            "Epoch 8/20, Loss: 0.6909\n",
            "Epoch 9/20, Loss: 0.6903\n",
            "Epoch 10/20, Loss: 0.6896\n",
            "Epoch 11/20, Loss: 0.6890\n",
            "Epoch 12/20, Loss: 0.6885\n",
            "Epoch 13/20, Loss: 0.6879\n",
            "Epoch 14/20, Loss: 0.6874\n",
            "Epoch 15/20, Loss: 0.6868\n",
            "Epoch 16/20, Loss: 0.6863\n",
            "Epoch 17/20, Loss: 0.6858\n",
            "Epoch 18/20, Loss: 0.6852\n",
            "Epoch 19/20, Loss: 0.6847\n",
            "Epoch 20/20, Loss: 0.6841\n",
            "Attack Model Validation Accuracy: 61.50%\n",
            "9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-2171e4d8ca2c>:370: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X = torch.tensor(X, dtype=torch.float32)  # Convert X to tensor\n",
            "<ipython-input-12-2171e4d8ca2c>:383: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X = torch.tensor(X, dtype=torch.float32)  # Convert X to tensor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 0.6959\n",
            "Epoch 2/20, Loss: 0.6953\n",
            "Epoch 3/20, Loss: 0.6947\n",
            "Epoch 4/20, Loss: 0.6940\n",
            "Epoch 5/20, Loss: 0.6935\n",
            "Epoch 6/20, Loss: 0.6929\n",
            "Epoch 7/20, Loss: 0.6924\n",
            "Epoch 8/20, Loss: 0.6919\n",
            "Epoch 9/20, Loss: 0.6915\n",
            "Epoch 10/20, Loss: 0.6910\n",
            "Epoch 11/20, Loss: 0.6906\n",
            "Epoch 12/20, Loss: 0.6902\n",
            "Epoch 13/20, Loss: 0.6898\n",
            "Epoch 14/20, Loss: 0.6894\n",
            "Epoch 15/20, Loss: 0.6891\n",
            "Epoch 16/20, Loss: 0.6887\n",
            "Epoch 17/20, Loss: 0.6883\n",
            "Epoch 18/20, Loss: 0.6879\n",
            "Epoch 19/20, Loss: 0.6875\n",
            "Epoch 20/20, Loss: 0.6872\n",
            "Attack Model Validation Accuracy: 58.00%\n",
            "y_true:[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
            "y_pred:[array(1., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(1., dtype=float32), array(1., dtype=float32), array(1., dtype=float32), array(1., dtype=float32), array(1., dtype=float32), array(1., dtype=float32), array(1., dtype=float32)]\n",
            "Attack Evaluation Results:\n",
            "accuracy: 0.4000\n",
            "precision: 0.4444\n",
            "recall: 0.8000\n",
            "f1_score: 0.5714\n",
            "roc_auc: 0.4000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
            "  warnings.warn(\n",
            "<ipython-input-12-2171e4d8ca2c>:463: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X = torch.tensor(X, dtype=torch.float32)  # Convert X to tensor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "mia = MembershipInferenceAttack(\n",
        "    target_model=target_model,\n",
        "    target_class=SimpleNN,\n",
        "    dataset=dataset,\n",
        "    data_distribution=data_distribution,\n",
        "    shadow_models_count=40,\n",
        "    attack_model_type=\"LiRA\",\n",
        "    device=device,\n",
        "    hyperparameters={\n",
        "        'num_perturbations': 50,\n",
        "        'epsilon': 0.01,\n",
        "        'augmentation_strategy': 'noise'\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "memb2 = torch.utils.data.Subset(members, list(range(5)))\n",
        "non_memb2 = torch.utils.data.Subset(non_members, list(range(5)))\n",
        "mia.prepare_datasets(memb2,non_memb2)\n",
        "\n",
        "\n",
        "mia.configure_attack_model()\n",
        "\n",
        "\n",
        "results = mia.evaluate_attack()\n",
        "\n",
        "\n",
        "print(\"Attack Evaluation Results:\")\n",
        "for metric, value in results.items():\n",
        "    print(f\"{metric}: {value:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Wpc4DJJyKa1o",
        "outputId": "205551ac-9d0d-4609-89d4-a23d46eec7e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuring LiRA attack...\n",
            "0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-792ee4d49eb3>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmia\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_attack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-c926ae4ba3f5>\u001b[0m in \u001b[0;36mevaluate_attack\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    670\u001b[0m         \"\"\"\n\u001b[1;32m    671\u001b[0m         \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmembers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnon_members\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_membership\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmembers\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnon_members\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0;31m# Compute evaluation metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-c926ae4ba3f5>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    670\u001b[0m         \"\"\"\n\u001b[1;32m    671\u001b[0m         \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmembers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnon_members\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_membership\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmembers\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnon_members\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0;31m# Compute evaluation metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-c926ae4ba3f5>\u001b[0m in \u001b[0;36minfer_membership\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattack_model_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'LiRA'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattack_model_type\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'GLiRA'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattack_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m7\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattack_model_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'MAST'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MALT'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-c926ae4ba3f5>\u001b[0m in \u001b[0;36m_lira_attack\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;31m# Run the LiRA attack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m         \u001b[0mlikelihood_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlira\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_attack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlikelihood_ratio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-c926ae4ba3f5>\u001b[0m in \u001b[0;36mrun_attack\u001b[0;34m(self, model, target_example)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;31m# Step 1: Collect IN and OUT confidences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mconfs_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_confidences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_distribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_example\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_in\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0mconfs_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_confidences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_distribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_example\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_in\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-c926ae4ba3f5>\u001b[0m in \u001b[0;36mcollect_confidences\u001b[0;34m(self, dataset, target_example, is_in)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mshadow_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# Convert data to tensors during dataset preparation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mshadow_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshadow_dataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-c926ae4ba3f5>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mshadow_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# Convert data to tensors during dataset preparation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mshadow_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshadow_dataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LiRA"
      ],
      "metadata": {
        "id": "DALx7hR0tlOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class LiRA:\n",
        "    def __init__(self, model_class, data_distribution, N=50):\n",
        "        \"\"\"\n",
        "        Initialize the LiRA attack.\n",
        "\n",
        "        Parameters:\n",
        "        - model_class: Class to initialize and train shadow models.\n",
        "        - data_distribution: Function to sample shadow datasets.\n",
        "        - N: Number of shadow models to train.\n",
        "        \"\"\"\n",
        "        self.model_class = model_class\n",
        "        self.data_distribution = data_distribution\n",
        "        self.N = N\n",
        "\n",
        "    def train_shadow_model(self, dataset, model):\n",
        "        \"\"\"Train a shadow model on the given dataset.\"\"\"\n",
        "        model.train()\n",
        "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        for epoch in range(5):  # Train for 5 epochs\n",
        "            for X, y in dataloader:\n",
        "                optimizer.zero_grad()\n",
        "                predictions = model(X)\n",
        "                loss = criterion(predictions, y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        return model\n",
        "\n",
        "    def collect_confidences(self, dataset, target_example, is_in=True):\n",
        "        confidences = []\n",
        "        x, y = target_example\n",
        "        def is_not_target(d, target):\n",
        "          return not (torch.equal(d[0], target[0]) and d[1] == target[1])\n",
        "\n",
        "\n",
        "        for _ in range(self.N):  # N should be >= 5 for meaningful variance\n",
        "            shadow_dataset = self.data_distribution()\n",
        "            # Convert data to tensors during dataset preparation\n",
        "            shadow_dataset = [(torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.long)) for x, y in shadow_dataset]\n",
        "\n",
        "            if is_in:\n",
        "                shadow_dataset.append(target_example)\n",
        "            else:\n",
        "                shadow_dataset = [d for d in shadow_dataset if is_not_target(d, target_example)]\n",
        "\n",
        "\n",
        "            # Train shadow model\n",
        "            shadow_model = self.model_class()\n",
        "            shadow_model = self.train_shadow_model(shadow_dataset, shadow_model)\n",
        "\n",
        "            # Get confidence of the target example\n",
        "            shadow_model.eval()\n",
        "            with torch.no_grad():\n",
        "                confidence = nn.Softmax(dim=1)(shadow_model(x.unsqueeze(0)))\n",
        "                confidences.append(confidence[0, y].item())\n",
        "\n",
        "        if len(confidences) < 2:\n",
        "            raise ValueError(\"Insufficient confidence values to compute meaningful statistics.\")\n",
        "        return confidences\n",
        "\n",
        "\n",
        "\n",
        "    def compute_statistics(self, confidences):\n",
        "        \"\"\"Compute mean and variance of confidence values.\"\"\"\n",
        "        mean = np.mean(confidences)\n",
        "        variance = np.var(confidences)\n",
        "        return mean, variance\n",
        "\n",
        "    def likelihood_ratio(self, conf_obs, mu_in, var_in, mu_out, var_out):\n",
        "        \"\"\"\n",
        "        Compute the likelihood ratio.\n",
        "\n",
        "        Parameters:\n",
        "        - conf_obs: Observed confidence value.\n",
        "        - mu_in, var_in: Mean and variance of IN distribution.\n",
        "        - mu_out, var_out: Mean and variance of OUT distribution.\n",
        "        \"\"\"\n",
        "        def gaussian_likelihood(x, mu, var):\n",
        "          epsilon = 1e-10  # Small constant to prevent zero variance\n",
        "          var = max(var, epsilon)  # Avoid division by zero\n",
        "          return (1 / np.sqrt(2 * np.pi * var)) * np.exp(-(x - mu) ** 2 / (2 * var))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        epsilon = 1e-10  # Small value to prevent division by zero\n",
        "        p_in = gaussian_likelihood(conf_obs, mu_in, var_in)\n",
        "        p_out = gaussian_likelihood(conf_obs, mu_out, var_out)\n",
        "        print(f\"conf_obs: {conf_obs}\")\n",
        "        print(f\"mu_in: {mu_in}, var_in: {var_in}\")\n",
        "        print(f\"mu_out: {mu_out}, var_out: {var_out}\")\n",
        "\n",
        "        return p_in / (p_out + epsilon)\n",
        "\n",
        "    def run_attack(self, model, target_example):\n",
        "        \"\"\"\n",
        "        Run the LiRA attack on the target example.\n",
        "\n",
        "        Parameters:\n",
        "        - model: Target model.\n",
        "        - target_example: Example to attack (x, y).\n",
        "        \"\"\"\n",
        "        # Step 1: Collect IN and OUT confidences\n",
        "        confs_in = self.collect_confidences(self.data_distribution(), target_example, is_in=True)\n",
        "        confs_out = self.collect_confidences(self.data_distribution(), target_example, is_in=False)\n",
        "\n",
        "        mu_in = np.mean(confs_in)\n",
        "        var_in = np.var(confs_in)\n",
        "\n",
        "        mu_out = np.mean(confs_out)\n",
        "        var_out = np.var(confs_out)\n",
        "\n",
        "        print(f\"Confs_in: {confs_in}, Confs_out: {confs_out}\")\n",
        "\n",
        "        # Step 2: Compute IN and OUT statistics\n",
        "        mu_in, var_in = self.compute_statistics(confs_in)\n",
        "        mu_out, var_out = self.compute_statistics(confs_out)\n",
        "\n",
        "        # Step 3: Query the target model\n",
        "        x, y = target_example\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            conf_obs = nn.Softmax(dim=1)(model(x.unsqueeze(0)))[0, y].item()\n",
        "\n",
        "        # Step 4: Compute likelihood ratio\n",
        "        likelihood_ratio = self.likelihood_ratio(conf_obs, mu_in, var_in, mu_out, var_out)\n",
        "        return likelihood_ratio\n"
      ],
      "metadata": {
        "id": "xueoHJ9dtkN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Define a simple neural network\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size=784, num_classes=10):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x.view(x.size(0), -1))\n",
        "\n",
        "# Define a function to generate shadow datasets\n",
        "def data_distribution():\n",
        "    \"\"\"\n",
        "    Generate a random shadow dataset from MNIST.\n",
        "    \"\"\"\n",
        "    dataset = datasets.MNIST(root=\"./data\", train=True, transform=transforms.ToTensor(), download=True)\n",
        "    indices = np.random.choice(len(dataset), 500, replace=False)  # Random 500 samples\n",
        "    subset = torch.utils.data.Subset(dataset, indices)\n",
        "    return list(subset)\n",
        "\n",
        "# Initialize and train the target model\n",
        "def train_target_model(model, dataset, epochs=5):\n",
        "    model.train()\n",
        "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            predictions = model(X)\n",
        "            loss = criterion(predictions, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    return model\n",
        "\n",
        "# Test the LiRA implementation\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Prepare MNIST dataset\n",
        "    mnist_train = datasets.MNIST(root=\"./data\", train=True, transform=transforms.ToTensor(), download=True)\n",
        "    target_dataset = torch.utils.data.Subset(mnist_train, list(range(1000)))  # Use 1000 samples for target model\n",
        "\n",
        "    # Train the target model\n",
        "    target_model = SimpleNN().to(device)\n",
        "    target_model = train_target_model(target_model, target_dataset)\n",
        "\n",
        "    # Choose a random target example\n",
        "    target_example = target_dataset[0]\n",
        "    x, y = target_example\n",
        "    x2, y2 = x.to(device), torch.tensor(y).to(device)\n",
        "\n",
        "    # Initialize LiRA\n",
        "    lira = LiRA(model_class=SimpleNN, data_distribution=data_distribution, N=10)\n",
        "\n",
        "    # Run the LiRA attack\n",
        "    likelihood_ratio = lira.run_attack(target_model, (x2, y2))\n",
        "\n",
        "    # Output the result\n",
        "    print(f\"Likelihood Ratio for the target example: {likelihood_ratio}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1Rq6VsXztXC",
        "outputId": "7fadf409-e12e-4bbf-e2f9-3c7c8418c8f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-29312b4af2d9>:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  shadow_dataset = [(torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.long)) for x, y in shadow_dataset]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confs_in: [0.538368821144104, 0.6882573962211609, 0.8102095127105713, 0.5609371662139893, 0.4878362715244293, 0.48510465025901794, 0.5002596974372864, 0.620842456817627, 0.4333582818508148, 0.6667373776435852], Confs_out: [0.353847861289978, 0.23967750370502472, 0.5822126865386963, 0.6040904521942139, 0.18072937428951263, 0.5089112520217896, 0.6811476945877075, 0.5957340002059937, 0.18828971683979034, 0.5291156768798828]\n",
            "conf_obs: 0.5130645632743835\n",
            "mu_in: 0.5791911631822586, var_in: 0.012135915665683727\n",
            "mu_out: 0.4463756218552589, var_out: 0.031996991065060126\n",
            "Likelihood Ratio for the target example: 1.453649397778496\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmI95vUm87dr",
        "outputId": "47d102a8-a86f-4290-f17f-469f9ab7cead"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GLiRA"
      ],
      "metadata": {
        "id": "UuLhu2WV0Nz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "class LikelihoodRatioAttack:\n",
        "    def __init__(self, model_class, data_distribution, N=50):\n",
        "        \"\"\"\n",
        "        Initialize the Likelihood Ratio Attack.\n",
        "\n",
        "        Parameters:\n",
        "        - model_class: A callable that initializes and trains shadow models.\n",
        "        - data_distribution: A callable that provides datasets for shadow models.\n",
        "        - N: Number of shadow models to train.\n",
        "        \"\"\"\n",
        "        self.model_class = model_class\n",
        "        self.data_distribution = data_distribution\n",
        "        self.N = N\n",
        "\n",
        "    def train_shadow_model(self, dataset):\n",
        "        \"\"\"Train a shadow model on the given dataset.\"\"\"\n",
        "        model = self.model_class()\n",
        "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        for epoch in range(5):  # Train for 5 epochs\n",
        "            for X, y in dataloader:\n",
        "                optimizer.zero_grad()\n",
        "                predictions = model(X)\n",
        "                loss = criterion(predictions, y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        return model\n",
        "\n",
        "    def collect_confidences(self, dataset, target_example, is_in):\n",
        "        \"\"\"\n",
        "        Collect confidence values for shadow models.\n",
        "\n",
        "        Parameters:\n",
        "        - dataset: The shadow dataset to use.\n",
        "        - target_example: The target example (x, y).\n",
        "        - is_in: Whether the example is part of the dataset (IN or OUT).\n",
        "        \"\"\"\n",
        "        confidences = []\n",
        "        x, y = target_example\n",
        "\n",
        "        for _ in range(self.N):\n",
        "            shadow_dataset = self.data_distribution()  # Sample shadow dataset\n",
        "            if is_in:\n",
        "                shadow_dataset.append(target_example)  # Add the target example\n",
        "            else:\n",
        "                shadow_dataset = [d for d in shadow_dataset if d != target_example]  # Remove the target example\n",
        "\n",
        "            shadow_model = self.train_shadow_model(shadow_dataset)\n",
        "            shadow_model.eval()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                confidence = nn.Softmax(dim=1)(shadow_model(x.unsqueeze(0)))\n",
        "                confidences.append(confidence[0, y].item())\n",
        "        return confidences\n",
        "\n",
        "    def compute_statistics(self, confidences):\n",
        "        \"\"\"Compute mean and variance of confidence scores.\"\"\"\n",
        "        mean = np.mean(confidences)\n",
        "        variance = np.var(confidences)\n",
        "        return mean, variance\n",
        "\n",
        "    def likelihood_ratio(self, conf_obs, mu_in, var_in, mu_out, var_out):\n",
        "        \"\"\"\n",
        "        Compute the likelihood ratio.\n",
        "\n",
        "        Parameters:\n",
        "        - conf_obs: Observed confidence value.\n",
        "        - mu_in, var_in: Mean and variance for \"IN\" distribution.\n",
        "        - mu_out, var_out: Mean and variance for \"OUT\" distribution.\n",
        "        \"\"\"\n",
        "        def gaussian_likelihood(x, mu, var):\n",
        "            return (1 / np.sqrt(2 * np.pi * var)) * np.exp(-(x - mu) ** 2 / (2 * var))\n",
        "\n",
        "        p_in = gaussian_likelihood(conf_obs, mu_in, var_in)\n",
        "        p_out = gaussian_likelihood(conf_obs, mu_out, var_out)\n",
        "        return p_in / p_out\n",
        "\n",
        "    def online_attack(self, model, target_example):\n",
        "        \"\"\"\n",
        "        Perform the online variant of the Likelihood Ratio Attack.\n",
        "\n",
        "        Parameters:\n",
        "        - model: The target model.\n",
        "        - target_example: The target input example (x, y).\n",
        "        \"\"\"\n",
        "        conf_in = self.collect_confidences(self.data_distribution(), target_example, is_in=True)\n",
        "        conf_out = self.collect_confidences(self.data_distribution(), target_example, is_in=False)\n",
        "\n",
        "        mu_in, var_in = self.compute_statistics(conf_in)\n",
        "        mu_out, var_out = self.compute_statistics(conf_out)\n",
        "\n",
        "        x, y = target_example\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            conf_obs = nn.Softmax(dim=1)(model(x.unsqueeze(0)))[0, y].item()\n",
        "\n",
        "        likelihood_ratio = self.likelihood_ratio(conf_obs, mu_in, var_in, mu_out, var_out)\n",
        "        return likelihood_ratio\n",
        "\n",
        "    def offline_attack(self, model, precomputed_stats, target_example):\n",
        "        \"\"\"\n",
        "        Perform the offline variant of the Likelihood Ratio Attack.\n",
        "\n",
        "        Parameters:\n",
        "        - model: The target model.\n",
        "        - precomputed_stats: Pre-computed statistics for \"IN\" and \"OUT\" distributions.\n",
        "        - target_example: The target input example (x, y).\n",
        "        \"\"\"\n",
        "        mu_in, var_in, mu_out, var_out = precomputed_stats\n",
        "\n",
        "        x, y = target_example\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            conf_obs = nn.Softmax(dim=1)(model(x.unsqueeze(0)))[0, y].item()\n",
        "\n",
        "        likelihood_ratio = self.likelihood_ratio(conf_obs, mu_in, var_in, mu_out, var_out)\n",
        "        return likelihood_ratio\n"
      ],
      "metadata": {
        "id": "vDOJ6Mv20NNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# shadow models"
      ],
      "metadata": {
        "id": "NOloVevwdHdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
        "\n",
        "class ShokriMembershipInference:\n",
        "    def __init__(self, shadow_model_class, data_distribution, num_shadow_models=5):\n",
        "        \"\"\"\n",
        "        Initialize the membership inference attack class.\n",
        "\n",
        "        Args:\n",
        "            shadow_model_class: A class to initialize shadow models.\n",
        "            data_distribution: A function to generate datasets for shadow models.\n",
        "            num_shadow_models: Number of shadow models to train.\n",
        "        \"\"\"\n",
        "        self.shadow_model_class = shadow_model_class\n",
        "        self.data_distribution = data_distribution\n",
        "        self.num_shadow_models = num_shadow_models\n",
        "        self.attack_model = None\n",
        "\n",
        "    def train_shadow_model(self, train_data):\n",
        "        \"\"\"\n",
        "        Train a single shadow model.\n",
        "\n",
        "        Args:\n",
        "            train_data: Dataset to train the shadow model.\n",
        "\n",
        "        Returns:\n",
        "            Trained shadow model.\n",
        "        \"\"\"\n",
        "        model = self.shadow_model_class()\n",
        "        model.train()\n",
        "        dataloader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        for epoch in range(5):\n",
        "            for X, y in dataloader:\n",
        "                optimizer.zero_grad()\n",
        "                predictions = model(X)\n",
        "                loss = criterion(predictions, y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        return model\n",
        "\n",
        "    def build_attack_dataset(self):\n",
        "        \"\"\"\n",
        "        Build the attack dataset by training shadow models and collecting their outputs.\n",
        "\n",
        "        Returns:\n",
        "            X_attack: Features for the attack model (e.g., confidence scores).\n",
        "            y_attack: Labels for the attack model (1 for members, 0 for non-members).\n",
        "        \"\"\"\n",
        "        X_attack = []\n",
        "        y_attack = []\n",
        "\n",
        "        for _ in range(self.num_shadow_models):\n",
        "            shadow_train, shadow_test = self.data_distribution()\n",
        "\n",
        "            # Train shadow model on shadow_train\n",
        "            shadow_model = self.train_shadow_model(shadow_train)\n",
        "\n",
        "            # Collect confidence scores for members (training set)\n",
        "            for X, y in DataLoader(shadow_train, batch_size=1):\n",
        "                shadow_model.eval()\n",
        "                with torch.no_grad():\n",
        "                    softmax_outputs = F.softmax(shadow_model(X), dim=1)\n",
        "                    confidence_score = softmax_outputs.max().item()\n",
        "                    X_attack.append([confidence_score])\n",
        "                    y_attack.append(1)  # Member\n",
        "\n",
        "            # Collect confidence scores for non-members (testing set)\n",
        "            for X, y in DataLoader(shadow_test, batch_size=1):\n",
        "                shadow_model.eval()\n",
        "                with torch.no_grad():\n",
        "                    softmax_outputs = F.softmax(shadow_model(X), dim=1)\n",
        "                    confidence_score = softmax_outputs.max().item()\n",
        "                    X_attack.append([confidence_score])\n",
        "                    y_attack.append(0)  # Non-member\n",
        "\n",
        "        return X_attack, y_attack\n",
        "\n",
        "    def train_attack_model(self):\n",
        "        \"\"\"\n",
        "        Train the attack model using the attack dataset.\n",
        "        \"\"\"\n",
        "        # Build the attack dataset\n",
        "        X_attack, y_attack = self.build_attack_dataset()\n",
        "\n",
        "        # Split into training and validation sets\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X_attack, y_attack, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Train a logistic regression attack model\n",
        "        attack_model = LogisticRegression()\n",
        "        attack_model.fit(X_train, y_train)\n",
        "\n",
        "        # Validate the attack model\n",
        "        y_pred = attack_model.predict(X_val)\n",
        "        accuracy = accuracy_score(y_val, y_pred)\n",
        "        print(f\"Attack Model Validation Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "        self.attack_model = attack_model\n",
        "\n",
        "    def perform_inference(self, target_model, sample):\n",
        "        \"\"\"\n",
        "        Perform membership inference on a sample using the trained attack model.\n",
        "\n",
        "        Args:\n",
        "            target_model: The target model to attack.\n",
        "            sample: The sample to perform inference on.\n",
        "\n",
        "        Returns:\n",
        "            Membership inference result (1 for member, 0 for non-member).\n",
        "        \"\"\"\n",
        "        if self.attack_model is None:\n",
        "            raise ValueError(\"Attack model has not been trained. Call train_attack_model() first.\")\n",
        "\n",
        "        target_model.eval()\n",
        "        with torch.no_grad():\n",
        "            softmax_outputs = F.softmax(target_model(sample[0].unsqueeze(0)), dim=1)\n",
        "            confidence_score = softmax_outputs.max().item()\n",
        "\n",
        "        # Use the attack model to predict membership\n",
        "        membership_prediction = self.attack_model.predict([[confidence_score]])[0]\n",
        "        return membership_prediction\n"
      ],
      "metadata": {
        "id": "2q551CPQdG3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple data distribution function\n",
        "def data_distribution():\n",
        "    X = torch.randn(1000, 784)\n",
        "    y = torch.randint(0, 10, (1000,))\n",
        "    dataset = TensorDataset(X, y)\n",
        "    train_size = int(0.5 * len(dataset))\n",
        "    test_size = len(dataset) - train_size\n",
        "    return random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Define a simple neural network\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size=784, num_classes=10):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Initialize and train the attack\n",
        "smi = ShokriMembershipInference(SimpleNN, data_distribution, num_shadow_models=5)\n",
        "smi.train_attack_model()\n",
        "\n",
        "# Test inference\n",
        "target_model = SimpleNN()\n",
        "target_sample = (torch.randn(784), 1)  # Example sample\n",
        "print(\"Membership Prediction:\", smi.perform_inference(target_model, target_sample))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuKKiIhedr-Y",
        "outputId": "b65c3e44-8cc6-412e-b704-b65fa89ddd34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attack Model Validation Accuracy: 98.90%\n",
            "Membership Prediction: 0\n"
          ]
        }
      ]
    }
  ]
}